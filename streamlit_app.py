# HÃœCRE 8: streamlit_app.py dosyasÄ±nÄ± oluÅŸturma (KURALA UYGUN VE SON VERSÄ°YON)
%%writefile streamlit_app.py
import streamlit as st
import os
import pandas as pd
from datasets import load_dataset, concatenate_datasets
from langchain_community.vectorstores import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain.chains import RetrievalQA
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders.dataframe import DataFrameLoader # Yeni eklendi

# --- KonfigÃ¼rasyonlar ve Sabitler ---
CHROMA_PATH = "cancer_pubmed_db"
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")

if not GEMINI_API_KEY:
    # Streamlit Cloud'da secret ayarÄ±nÄ± kontrol eder.
    st.error("GEMINI_API_KEY ortam deÄŸiÅŸkeni bulunamadÄ±. LÃ¼tfen Streamlit Secrets'Ä± kontrol edin.")
    st.stop()

def create_and_persist_vector_db():
    """VeritabanÄ± yoksa HuggingFace'ten veriyi Ã§ekip ChromaDB'yi yeniden oluÅŸturur."""
    st.info("VeritabanÄ± bulunamadÄ±. ChromaDB, Hugging Face verileri kullanÄ±larak yeniden oluÅŸturuluyor...")

    # 1. Veri Setlerini YÃ¼kleme ve KÄ±sÄ±tlama (1000 satÄ±r)
    # Bu, Streamlit Cloud'un hÄ±zlÄ± Ã§alÄ±ÅŸmasÄ± iÃ§in kritiktir.
    breast_data = load_dataset("Gaborandi/breast_cancer_pubmed_abstracts", split="train") 
    lung_data = load_dataset("Gaborandi/Lung_Cancer_pubmed_abstracts", split="train") 
    combined_data = concatenate_datasets([breast_data, lung_data])
    df = combined_data.to_pandas()
    df = df.sample(n=1000, random_state=42) # 1000 satÄ±rla sÄ±nÄ±rla
    df = df[['abstract', 'title']].dropna() 

    # 2. Chunking ve DokÃ¼manlara Ã‡evirme
    loader = DataFrameLoader(df, page_content_column="abstract")
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
    chunks = text_splitter.split_documents(documents)
    
    # 3. Embedding Modelini YÃ¼kleme
    embeddings = GoogleGenerativeAIEmbeddings(
        model="text-embedding-004",
        google_api_key=GEMINI_API_KEY
    )
    
    # 4. ChromaDB'yi OluÅŸturma
    vector_store = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        persist_directory=CHROMA_PATH
    )
    # Persist etmeye gerek yok, Streamlit Cloud'da yeniden oluÅŸturulacaktÄ±r.
    st.success(f"ChromaDB, {len(chunks)} parÃ§a ile baÅŸarÄ±yla yeniden oluÅŸturuldu.")
    return vector_store

@st.cache_resource
def setup_rag_chain():
    """RAG zincirini kurar ve dÃ¶ndÃ¼rÃ¼r."""
    embeddings = GoogleGenerativeAIEmbeddings(
        model="text-embedding-004",
        google_api_key=GEMINI_API_KEY
    )

    # 1. VeritabanÄ±nÄ± YÃ¼kle veya Yeniden OluÅŸtur (GitHub'a yÃ¼klenmediÄŸi iÃ§in)
    if not os.path.exists(CHROMA_PATH):
        # Dosya yoksa yeniden oluÅŸtur
        vector_store = create_and_persist_vector_db()
    else:
        # Dosya varsa yÃ¼kle (Bu kod sadece Colab'de Ã§alÄ±ÅŸÄ±rken tetiklenir)
        vector_store = Chroma(
            persist_directory=CHROMA_PATH,
            embedding_function=embeddings
        )
    
    # 2. LLM ve Retriever'Ä± Kur
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        temperature=0.0,
        google_api_key=GEMINI_API_KEY
    )
    
    retriever = vector_store.as_retriever(search_kwargs={"k": 3})
    
    # 3. RAG Zincirini OluÅŸtur
    qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever, return_source_documents=True)
    return qa_chain

# --- Streamlit ArayÃ¼zÃ¼ (AynÄ± kalÄ±r) ---
st.set_page_config(page_title="CancerLit-RAG Chatbot", layout="wide")
st.title("ğŸ§¬ CancerLit-RAG: Kanser Biyolojisi Chatbot")
st.markdown("PubMed makale Ã¶zetleri Ã¼zerine kurulmuÅŸ, **Gemini** destekli $\text{RAG}$ (Retrieval Augmented Generation) sistemi. Spesifik sorularÄ±nÄ±za bilimsel ve kaynak destekli cevaplar saÄŸlar.")

qa_chain = setup_rag_chain()

# Sohbet geÃ§miÅŸi ve kullanÄ±cÄ± giriÅŸi (Ã¶nceki gibi devam eder...)
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("Kanser biyolojisi veya genetiÄŸi hakkÄ±nda bir soru sorun (Ã–rn: BRCA1 mutasyonu nedir?)..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.spinner("Bilimsel literatÃ¼r taranÄ±yor ve cevap Ã¼retiliyor..."):
        result = qa_chain.invoke({"query": prompt})
        response = result['result']
        sources = result['source_documents']
        
        with st.chat_message("assistant"):
            st.markdown(response)
            
            if sources:
                st.subheader("ğŸ’¡ Cevap KaynaklarÄ± (Ä°lk 3 Kaynak):")
                for i, doc in enumerate(sources):
                    title = doc.metadata.get('title', 'BaÅŸlÄ±k Yok')
                    st.markdown(f"**{i+1}. Kaynak BaÅŸlÄ±ÄŸÄ±:** {title[:150]}...")

    st.session_state.messages.append({"role": "assistant", "content": response})
