import streamlit as st
import os

from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_google_genai import GoogleGenerativeAIEmbeddings

# --- KonfigÃ¼rasyonlar ---
# Colab'de tanÄ±mlanan API anahtarÄ±nÄ± Streamlit uygulamasÄ±na yÃ¼kleme
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")

if not GEMINI_API_KEY:
    st.error("GEMINI_API_KEY ortam deÄŸiÅŸkeni bulunamadÄ±. LÃ¼tfen Colab Secrets'Ä± kontrol edin.")
    st.stop()

# ChromaDB veritabanÄ± yolu (Colab'de oluÅŸturduÄŸumuz klasÃ¶r)
CHROMA_PATH = "cancer_pubmed_db"

@st.cache_resource
def setup_rag_chain():
    """RAG zincirini kurar ve dÃ¶ndÃ¼rÃ¼r."""
    # 1. Embedding Modelini YÃ¼kle
    embeddings = GoogleGenerativeAIEmbeddings(
        model="text-embedding-004",
        google_api_key=GEMINI_API_KEY
    )
    
    # 2. VektÃ¶r VeritabanÄ±nÄ± YÃ¼kle
    if not os.path.exists(CHROMA_PATH):
        st.error("ChromaDB veritabanÄ± dosyalarÄ± ('cancer_pubmed_db') bulunamadÄ±. LÃ¼tfen 5. HÃ¼creyi (Embedding) Ã§alÄ±ÅŸtÄ±rÄ±n ve dosyalarÄ± GitHub'a yÃ¼kleyin.")
        st.stop()
        
    vector_store = Chroma(
        persist_directory=CHROMA_PATH,
        embedding_function=embeddings
    )
    
    # 3. LLM ve Retriever'Ä± Kur
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        temperature=0.0,
        google_api_key=GEMINI_API_KEY
    )
    
    retriever = vector_store.as_retriever(search_kwargs={"k": 3})
    
    # 4. RAG Zincirini OluÅŸtur
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True
    )
    return qa_chain

# --- Streamlit ArayÃ¼zÃ¼ ---

st.set_page_config(page_title="CancerLit-RAG Chatbot", layout="wide")
st.title("ğŸ§¬ CancerLit-RAG: Kanser Biyolojisi Chatbot")
st.markdown("PubMed makale Ã¶zetleri Ã¼zerine kurulmuÅŸ, **Gemini** destekli $\text{RAG}$ (Retrieval Augmented Generation) sistemi. Spesifik sorularÄ±nÄ±za bilimsel ve kaynak destekli cevaplar saÄŸlar.")

# RAG zincirini bir kez kur
qa_chain = setup_rag_chain()

# Sohbet geÃ§miÅŸini baÅŸlat
if "messages" not in st.session_state:
    st.session_state.messages = []

# Sohbet geÃ§miÅŸini gÃ¶ster
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# KullanÄ±cÄ± giriÅŸi
if prompt := st.chat_input("Kanser biyolojisi veya genetiÄŸi hakkÄ±nda bir soru sorun (Ã–rn: BRCA1 mutasyonu nedir?)..."):
    # KullanÄ±cÄ± mesajÄ±nÄ± geÃ§miÅŸe ekle
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.spinner("Bilimsel literatÃ¼r taranÄ±yor ve cevap Ã¼retiliyor..."):
        # RAG zincirini Ã§aÄŸÄ±r
        result = qa_chain.invoke({"query": prompt})
        response = result['result']
        sources = result['source_documents']
        
        # CevabÄ± gÃ¶ster
        with st.chat_message("assistant"):
            st.markdown(response)
            
            # KaynaklarÄ± gÃ¶ster (Proje gereksinimi iÃ§in kritik)
            if sources:
                st.subheader("ğŸ’¡ Cevap KaynaklarÄ± (Ä°lk 3 Kaynak):")
                for i, doc in enumerate(sources):
                    title = doc.metadata.get('title', 'BaÅŸlÄ±k Yok')
                    st.markdown(f"**{i+1}. Kaynak BaÅŸlÄ±ÄŸÄ±:** {title[:150]}...")
                    #st.caption(f"Ä°Ã§erik ParÃ§asÄ±: {doc.page_content[:200]}...")

    # Asistan cevabÄ±nÄ± geÃ§miÅŸe ekle
    st.session_state.messages.append({"role": "assistant", "content": response})
